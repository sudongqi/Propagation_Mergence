{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "#load phrase mining result (First half of TopMine)\n",
    "lines=[]\n",
    "with open(\"data/corpus.txt\") as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate double space\n",
    "def process_double_space(phrase):\n",
    "    new_phrase=re.split(' ', phrase)\n",
    "    if('' in new_phrase):\n",
    "        new_phrase.remove('')\n",
    "    new_phrase = ' '.join(new_phrase)\n",
    "    return new_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a ditionary for frequency\n",
    "phrase_freq={}\n",
    "phrase_list=[]\n",
    "phrase_index={}\n",
    "\n",
    "\n",
    "count=0\n",
    "for line in lines:\n",
    "    phrases=re.split('\\n|,',line.replace('-',' '))\n",
    "    phrases.remove('')\n",
    "    sub_list=[]\n",
    "    for phrase in phrases:\n",
    "        new_phrase=process_double_space(phrase)\n",
    "        if not new_phrase in phrase_index:\n",
    "            phrase_index[new_phrase]=count\n",
    "            count+=1\n",
    "        if new_phrase in phrase_freq:\n",
    "            phrase_freq[new_phrase]+=1\n",
    "        else:\n",
    "            phrase_freq[new_phrase]=1\n",
    "        sub_list.append(new_phrase)\n",
    "    phrase_list.append(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4463\n",
      "4463\n"
     ]
    }
   ],
   "source": [
    "# check size of dictionary\n",
    "print(len(phrase_freq))\n",
    "print(len(phrase_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'a b', 'b c', 'a b c']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function for break down long phrase\n",
    "def regroup_phrase(phrase):\n",
    "    group=[]\n",
    "    new_phrase=re.split(' ', phrase)\n",
    "    max_size=len(new_phrase)    \n",
    "    for i in range(0, max_size):\n",
    "        for j in range(0, len(new_phrase)-i):\n",
    "            new_string=new_phrase[j]\n",
    "            for k in range(i):\n",
    "                new_string+=' '\n",
    "                new_string+=new_phrase[j+k+1]\n",
    "            group.append(new_string)\n",
    "    return group\n",
    "            \n",
    "regroup_phrase('a b c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4463\n",
      "4463\n",
      "1554\n"
     ]
    }
   ],
   "source": [
    "#break long phrase into more sub phrases, as long as sub phrases is in the original dictionary\n",
    "import math\n",
    "import numpy\n",
    "\n",
    "super_phrase_list=[]\n",
    "super_phrase_freq={}\n",
    "phrase_docfreq={}\n",
    "phrase_idf=numpy.zeros(len(phrase_index))\n",
    "\n",
    "for line in phrase_list:\n",
    "    temp=set()\n",
    "    sub_list=[]\n",
    "    for phrase in line:\n",
    "        group=regroup_phrase(phrase)\n",
    "        for i in group:\n",
    "            if i in phrase_index:\n",
    "                temp.add(i)\n",
    "                sub_list.append(i)\n",
    "                if(i in super_phrase_freq):\n",
    "                    super_phrase_freq[i]+=1\n",
    "                else:\n",
    "                    super_phrase_freq[i]=1\n",
    "    for i in temp:\n",
    "        if i in phrase_docfreq:\n",
    "            phrase_docfreq[i]+=1\n",
    "        else:\n",
    "            phrase_docfreq[i]=1\n",
    "    super_phrase_list.append(sub_list)\n",
    "    \n",
    "#calculate Idf\n",
    "for key in phrase_docfreq:\n",
    "    count=phrase_docfreq[key]\n",
    "    phrase_idf[phrase_index[key]]=math.log(len(super_phrase_list)/float(count),2)\n",
    "\n",
    "#check size of dicitonary and size of document\n",
    "print(len(super_phrase_freq))\n",
    "print(len(phrase_docfreq))\n",
    "print(len(super_phrase_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate Tf-Idf for all papers\n",
    "texts_vec=[]\n",
    "\n",
    "for line in super_phrase_list:\n",
    "    vec=numpy.zeros(len(phrase_index))   \n",
    "    #accumulate local count\n",
    "    for p in line:\n",
    "        vec[phrase_index[p]]+=1        \n",
    "    #calculate tf\n",
    "    vec/=float(len(line))\n",
    "    #get tf-idf by multiplying idf\n",
    "    vec*=phrase_idf\n",
    "    texts_vec.append(vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00300707728668\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#similarity function = cosine distance\n",
    "from numpy import linalg as la\n",
    "def get_similarity(a,b):\n",
    "    return numpy.dot(a,b)/(la.norm(a)*la.norm(b))\n",
    "\n",
    "print get_similarity(texts_vec[0],texts_vec[8])\n",
    "print get_similarity(texts_vec[0],texts_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add similarity information into json\n",
    "with open(\"data/Paper_2014_clean.json\", \"r\") as f:\n",
    "    p_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This study used finite element models to assess potential benefits of selected unconventional features, implemented in an experimental car, for vehicle crashworthiness in frontal impact. These safety features include: structural energy-absorbing bumper, hood lockdown with optimized hood and extendable bumper. The A-pillar intrusion and the effective acceleration of the vehicle were used as the parameters for measuring frontal impact crashworthiness performance.\n",
      "['study', 'finite', 'element', 'model', 'finite element', 'element model', 'finite element model', 'assessment', 'potential', 'benefits', 'selection', 'unconventional', 'features', 'implementation', 'experimental', 'car', 'vehicle', 'crashworthiness', 'frontal', 'impact', 'safety', 'features', 'including', 'structure', 'hood', 'optimization', 'hood', 'extended', 'intrusion', 'effectiveness', 'acceleration', 'vehicle', 'parameters', 'measure', 'frontal', 'impact', 'crashworthiness', 'performance', 'math', 'based', 'math based', 'Performance Evaluation', 'experimental', 'car', 'frontal', 'impact', 'crashworthiness', 'math', 'based', 'math based', 'Performance Evaluation', 'experimental', 'car', 'frontal', 'impact', 'crashworthiness', 'math', 'based', 'math based', 'Performance Evaluation', 'experimental', 'car', 'frontal', 'impact', 'crashworthiness']\n"
     ]
    }
   ],
   "source": [
    "# make sure the index of json match the index of our result\n",
    "print p_data[1553]['abstract']\n",
    "print super_phrase_list[1553]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the similarity score for citaitons and add it into json\n",
    "for p in p_data:\n",
    "    \n",
    "    idx=int(p['index'])\n",
    "    cite_sim=[]\n",
    "    cite_by_sim=[]\n",
    "    all_cite_sim=[]\n",
    "    \n",
    "    for i in p['citations']:\n",
    "        s=get_similarity(texts_vec[idx],texts_vec[int(i)])\n",
    "        cite_sim.append(s)\n",
    "        all_cite_sim.append(s)\n",
    "        \n",
    "    for i in p['cited_by']:\n",
    "        s=get_similarity(texts_vec[idx],texts_vec[int(i)])\n",
    "        cite_by_sim.append(s)\n",
    "        all_cite_sim.append(s)\n",
    "    \n",
    "    \n",
    "    #json doesn't support numpy array\n",
    "    #put all similarity score into json\n",
    "    p['citations_sim']=cite_sim\n",
    "    p['cited_by_sim']=cite_by_sim\n",
    "    p['all_cite_sim']=all_cite_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most structural products have complex geometry to meet customerâ€™s demand of high functionality. Since manufacturing those products in one piece is either impossible or uneconomical, most structural products are assemblies of components with simpler geometries. The conventional way to design structural assemblies is to design overall geometry first, and then decompose the geometry to determine the part boundary and joint locations. This two-step process, however, can lead to sub-optimal designs since the product geometry, even if optimized as one piece, would not be optimal after decomposition. This paper presents a method for synthesizing structural assemblies directly from the design specifications, without going through the two-step process. Given an extended design domain with boundary and loading conditions, the method simultaneously optimizes the topology and geometry of an entire structure and the location and configuration of joints, considering structural performance, manufacturability, and assembleability. As a relaxation of our previous work utilizing a beam-based ground structure [1], this paper presents a new formulation in a continuum design domain, which greatly enhances the ability to represent complex structural geometry observed in real-world products. A multi-objective genetic algorithm is used to obtain Pareto optimal solutions that exhibits trade-offs among stiffness, weight, manufacturability, and assembleability.\n",
      "\n",
      " \n",
      "Level-set approaches are a family of domain classification techniques that rely on defining a scalar level-set function (LSF), then carrying out the classification based on the value of the function relative to one or more thresholds. Most continuum topology optimization formulations are at heart, a classification problem of the design domain into structural materials and void. As such, level-set approaches are gaining acceptance and popularity in structural topology optimization. In conventional level set approaches, finding an optimum LSF involves solution of a Hamilton-Jacobi system of partial differential equations with a large number of degrees of freedom, which in turn, cannot be accomplished without gradients information of the objective being optimized. A new approach is proposed in this paper where design variables are defined as the explicit values of the LSF at knot points, then a Kriging model is used to interpolate the LSF values within the rest of the domain so that classification into material or void can be performed. Perceived advantages of the explicit level-set (ELS) approach include alleviating the need for gradients of objectives and constraints, while maintaining a reasonable number of design variables that is independent from the mesh size. A hybrid genetic algorithm (GA) is then used for solving the optimization problem(s). An example problem of a short cantilever is studied under various settings of the ELS parameters in order to infer the best practice recommendations for tuning the approach. Capabilities of the approach are then further demonstrated by exploring its performance on several test problems.\n",
      "Application-Tailored Optimization Methods\n",
      " \n",
      "Level-set methods are domain classification techniques that are gaining popularity in the recent years for structural topology optimization. Level sets classify a domain into two or more categories (such as material and void) by examining the value of a scalar level-set function (LSF) defined in the entire design domain. In most level-set formulations, a large number of design variables, or degrees of freedom is used to define the LSF, which implicitly defines the structure. The large number of design variables makes non-gradient optimization techniques all but ineffective. Kriging-interpolated level sets (KLS) on the other hand are formulated with an objective to enable non-gradient optimization by defining the design variables as the LSF values at few select locations (knot points) and using a Kriging model to interpolate the LSF in the rest of the design domain. A downside of concern when adopting KLS, is that using too few knot points may limit the capability to represent complex shapes, while using too many knot points may cause difficulty for non-gradient optimization. This paper presents a study of the effect of number and layout of the knot points in KLS on the capability to represent complex topologies in single and multi-component structures. Image matching error metrics are employed to assess the degree of mismatch between target topologies and those best-attainable via KLS. Results are presented in a catalogue-style in order to facilitate appropriate selection of knot-points by designers wishing to apply KLS for topology optimization.\n",
      "Application-Tailored Optimization Methods\n",
      " \n",
      "[u'803', u'108']\n",
      "[0.074864227709044301, 0.54136390292430325]\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "p=p_data[1]\n",
    "print p_data[803]['abstract']\n",
    "print p_data[803]['broad_topic']\n",
    "print ' '\n",
    "print p_data[108]['abstract']\n",
    "print p_data[108]['broad_topic']\n",
    "print ' '\n",
    "print p['abstract']\n",
    "print p['broad_topic']\n",
    "print ' '\n",
    "print p['citations']\n",
    "print p['citations_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get phrase features for classification step\n",
    "for p in p_data:\n",
    "    \n",
    "    i=int(p['index'])\n",
    "    \n",
    "    vec=texts_vec[i]\n",
    "    z=sum(vec)\n",
    "    vec/=z\n",
    "    vec*=len(super_phrase_list[i])\n",
    "    word_bag={}\n",
    "    \n",
    "    for j in super_phrase_list[i]:\n",
    "        word_bag[str(phrase_index[j])]=vec[phrase_index[j]]\n",
    "    \n",
    "    p['phrases']=word_bag\n",
    "    p['phrases_size']=len(super_phrase_list[i])\n",
    "    \n",
    "index_phrase={}\n",
    "for key in phrase_index:\n",
    "    index_phrase[phrase_index[key]]=key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level set\n",
      "{'0': 0.9339053662077913,\n",
      " '1': 10.759764765407283,\n",
      " '10': 1.3113127542104279,\n",
      " '1063': 1.0858289956121243,\n",
      " '11': 1.3924718729894507,\n",
      " '12': 0.39921368386123585,\n",
      " '13': 1.245826129223383,\n",
      " '14': 0.66861008191347704,\n",
      " '148': 0.13901432944488251,\n",
      " '15': 4.80960966668406,\n",
      " '16': 1.4992461769088636,\n",
      " '17': 1.0504948227808346,\n",
      " '18': 1.1021721405981735,\n",
      " '19': 1.3358475686034921,\n",
      " '2': 0.93510152842312977,\n",
      " '20': 4.4773586025693666,\n",
      " '21': 4.9670361122383335,\n",
      " '22': 1.591756459469672,\n",
      " '23': 1.9548439881130657,\n",
      " '24': 1.1982488153974169,\n",
      " '25': 1.1135204182619747,\n",
      " '26': 1.8916376523928693,\n",
      " '263': 4.0084507323658407,\n",
      " '27': 0.96977185627747253,\n",
      " '28': 1.1019208548374571,\n",
      " '29': 0.71900984511049382,\n",
      " '3': 6.1484370088041613,\n",
      " '30': 0.43187125512703067,\n",
      " '31': 0.91087322529178505,\n",
      " '32': 0.5465678915762201,\n",
      " '33': 0.84198359100644438,\n",
      " '34': 0.54008356591118112,\n",
      " '35': 1.3217879439255495,\n",
      " '36': 0.82180483686758976,\n",
      " '362': 0.70222733758398315,\n",
      " '37': 1.7268685283242584,\n",
      " '38': 1.0318371780482614,\n",
      " '39': 8.5103864028251763,\n",
      " '4': 0.80338857099165084,\n",
      " '40': 1.8411079527252006,\n",
      " '41': 1.8885483845798636,\n",
      " '42': 1.1374191276677916,\n",
      " '43': 0.35234483398418648,\n",
      " '44': 5.5676020913098734,\n",
      " '45': 0.33726633707155701,\n",
      " '46': 7.5047480564780251,\n",
      " '5': 14.89317620494406,\n",
      " '502': 4.3923272109409295,\n",
      " '6': 0.72265070125573161,\n",
      " '7': 1.3437579410638525,\n",
      " '731': 1.1565129246760255,\n",
      " '76': 1.6658067369723015,\n",
      " '8': 2.5015826854926755,\n",
      " '9': 1.0702346597368226,\n",
      " '98': 3.9569181634880941,\n",
      " '995': 2.6679034314271739}\n",
      "130\n",
      "In this paper, we propose a level-set based topology optimization method for designing a reactor, which is used as a part of the DC-DC converter in electric and hybrid vehicles. Since it realizes a high-power driving motor and its performance relies on its component, i.e., reactor core, it is valuable to establish a reasonable design method for the reactor core. Boundary tracking type level-set topology optimization is suitable for this purpose, because the shape and topology of the target structure is clearly represented by the zero boundary of the level-set function, and the state variables are accurately computed using the zero boundary tracking mesh. We formulate the design problem on the basis of electromagnetics, and derive the design sensitivities. The derived sensitivities are linked with boundary tracking type level-set topology optimization, and as a result, a useful structural optimization method for the reactor core design problem is developed.\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "from pprint import pprint\n",
    "\n",
    "print index_phrase[1]\n",
    "\n",
    "pprint (p_data[0]['phrases'])\n",
    "print p_data[0]['phrases_size']\n",
    "print (p_data[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "super_data={}\n",
    "super_data['papers']=p_data\n",
    "super_data['index_phrase']=index_phrase\n",
    "\n",
    "\n",
    "path=\"data/super_data.json\"\n",
    "if(os.path.isfile(path)):\n",
    "    os.remove(path)\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(super_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
