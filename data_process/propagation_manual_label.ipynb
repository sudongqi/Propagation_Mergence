{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/idx_label.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8cf22dad6b07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#get label dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/idx_label.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0midx_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/idx_label.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from Queue import Queue\n",
    "\n",
    "#get network data\n",
    "with open(\"data/Paper_2014_clean_2.json\", \"r\") as f:\n",
    "    p_data = json.load(f)\n",
    "\n",
    "#get label dictionary\n",
    "with open(\"data/idx_label.json\", \"r\") as f:\n",
    "    idx_label = json.load(f)\n",
    "    \n",
    "#get source labels\n",
    "with open(\"data/source.json\", \"r\") as f:\n",
    "    source = json.load(f)\n",
    "    \n",
    "with open(\"data/index_phrase.json\", \"r\") as f:\n",
    "    index_phrase = json.load(f)\n",
    "    \n",
    "topics=np.zeros((len(p_data),len(idx_label)),dtype=float)\n",
    "# the size of label vector\n",
    "print len(idx_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# list for storing the index of all sources\n",
    "source_idx=[]\n",
    "# check set for preventing duplicate adding\n",
    "check=set()\n",
    "\n",
    "# citation mode = 'all_cite' or regular 'cited_by'\n",
    "# all_cite treat all citations as undirected connection, whereas cited_by is directed\n",
    "\n",
    "\n",
    "#build source_idx\n",
    "for key in source:\n",
    "    source_idx.append(int(key))\n",
    "    check.add(int(key))\n",
    "    for i in source[key]:\n",
    "        topics[int(key), i[0]]=i[1]\n",
    "\n",
    "        \n",
    "# BFS for find legit order\n",
    "ordering=[]\n",
    "\n",
    "#build depth 1 nodes, depth measure how far this nodes is from the sources\n",
    "depth_1=[]\n",
    "for i in source_idx:\n",
    "    for j in p_data[i]['all_cite']:\n",
    "        if not int(j) in check:\n",
    "            depth_1.append(int(j))\n",
    "            check.add(int(j))\n",
    "        \n",
    "ordering.append(depth_1)\n",
    "\n",
    "#recursively build all nodes\n",
    "running=True\n",
    "depth=0\n",
    "\n",
    "while running:\n",
    "    new_layer=[]\n",
    "    for i in ordering[depth]:\n",
    "        for j in p_data[i]['all_cite']:\n",
    "            if not int(j) in check:\n",
    "                check.add(int(j))\n",
    "                new_layer.append(int(j))\n",
    "    if(len(new_layer)==0):\n",
    "        running=False\n",
    "    else:\n",
    "        ordering.append(new_layer)\n",
    "        depth+=1\n",
    "\n",
    "# number of nodes in network\n",
    "print len(check)\n",
    "# depth of the tree\n",
    "print len(ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build edge weight by normalization\n",
    "weights=[]\n",
    "originality=0.05\n",
    "\n",
    "for p in p_data:\n",
    "    z = float(sum(p['all_cite_sim'])) + originality\n",
    "    vec=np.array(p['all_cite_sim'])/z\n",
    "    p['prop_ratio']=1-(originality/z)\n",
    "    weights.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n",
      "0.359067022867\n",
      "386\n",
      "0.105726548266\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# turn all str in citation into int vector\n",
    "for p in p_data:\n",
    "    int_vec=[]\n",
    "    for i in p['all_cite']:\n",
    "        int_vec.append(int(i))\n",
    "    p['edge_set']=int_vec\n",
    "\n",
    "# build and initilize topics matrix\n",
    "topics=np.zeros((len(p_data),len(idx_label)),dtype=float)\n",
    "for key in source:\n",
    "    for i in source[key]:\n",
    "        topics[int(key), i[0]]=i[1]\n",
    "\n",
    "    \n",
    "# propagation and mixing\n",
    "iter_num=300\n",
    "for i in range(iter_num):\n",
    "    for layer in ordering:\n",
    "        for node in layer:\n",
    "            n_vec = p_data[node]['edge_set']\n",
    "            sub = topics[n_vec, :]\n",
    "            update = np.dot(weights[node], sub)\n",
    "            topics[node,:]=update\n",
    "            \n",
    "for i in range(len(p_data)):\n",
    "    p_data[i]['actual_ratio']=np.sum(topics[i])\n",
    "    \n",
    "actual_ratios=[]\n",
    "prop_ratios=[]\n",
    "for p in p_data:\n",
    "    if p['actual_ratio']!=0.0:\n",
    "        actual_ratios.append(p['actual_ratio'])\n",
    "    if p['prop_ratio']!=0.0:\n",
    "        prop_ratios.append(p['prop_ratio'])\n",
    "\n",
    "        \n",
    "# prop_ratios contain node that has a least one undirected connection      \n",
    "print len(prop_ratios)\n",
    "print sum(prop_ratios)/len(prop_ratios)\n",
    "\n",
    "# actual_ratios contain no 0\n",
    "print len(actual_ratios)\n",
    "# actual ratios being \n",
    "print sum(actual_ratios)/len(actual_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple Naive Bayes classifier that deal with mulitple label\n",
    "# initilize count with additive smoothing (count+a)/(count+a*d)\n",
    "alpha=1\n",
    "word_count=[[alpha for i in range(len(index_phrase))] for j in range(len(idx_label))]\n",
    "topic_count=[alpha*(len(index_phrase)+1) for i in range(len(idx_label))]\n",
    "\n",
    "for i in check:\n",
    "    vec = p_data[i]['phrases']\n",
    "    vec_sum = p_data[i]['phrases_size']\n",
    "    # actual word_count * tospics_proportion\n",
    "    for j in range(len(topics[i])):\n",
    "        topic_count[j] += vec_sum * topics[i][j]\n",
    "        for k in vec:\n",
    "            word_count[j][int(k)] += vec[k] * topics[i][j]\n",
    "            \n",
    "# learning stage: calculating p(word|topic)\n",
    "word_proba=np.array(word_count)\n",
    "for i in range (len(idx_label)):\n",
    "    word_proba[i]/=topic_count[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediting stage (this stage can be written into fast numpy code)\n",
    "# likelihood (word appear 3.5 times) = log(p^3.5) = 3.5 *log p\n",
    "from math import log\n",
    "\n",
    "prediction=[]\n",
    "for p in p_data:\n",
    "    vec = p['phrases']\n",
    "    result=[]\n",
    "    for i in range (len(idx_label)):\n",
    "        likelihood=0\n",
    "        for j in vec:\n",
    "            likelihood+=log(word_proba[i][int(j)])*vec[j]\n",
    "        result.append(likelihood)\n",
    "    prediction.append(result)\n",
    "\n",
    "    \n",
    "    \n",
    "# predition stage 2 (this is in numpy)\n",
    "# renormalize\n",
    "# can choose to filter out the low value (<1e-12) to prevent underflow problem. \n",
    "# However, it won't hurt to keep the underflow problem\n",
    "\n",
    "pred_num=np.array(prediction)\n",
    "\n",
    "for i in range(len(p_data)):\n",
    "    pred_num[i]-=np.amax(pred_num[i])\n",
    "    pred_num[i]=np.exp(pred_num[i])\n",
    "    pred_num[i]/=np.sum(pred_num[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine propagation result and classification result\n",
    "combine_result=[]\n",
    "final_precision=1e-3\n",
    "\n",
    "for i in range (len(p_data)):\n",
    "    ratio=p_data[i]['actual_ratio']\n",
    "    combine_vec=topics[i]*ratio\n",
    "    combine_vec+=pred_num[i]*(1-ratio)\n",
    "    combine_vec/=np.sum(combine_vec)\n",
    "    combine_vec[combine_vec<final_precision]=0.0\n",
    "    combine_vec/=np.sum(combine_vec)\n",
    "    combine_result.append(combine_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  0.]\n",
      "[ 0.99683123  0.          0.00316877]\n",
      " \n",
      "preference_analysis\n",
      "4280.47326785\n",
      "210\n",
      " \n",
      "algorithm_probabilistic_approach\n",
      "6951.97470051\n",
      "949\n",
      " \n",
      "product_system_design\n",
      "5259.0992124\n",
      "727\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# output topics to p_data\n",
    "present_threshold=0.1\n",
    "output_count=[0 for i in range(len(idx_label))]\n",
    "\n",
    "for i in range(len(p_data)):\n",
    "    topic_set=[]\n",
    "    for j in range(len(idx_label)): \n",
    "        if(combine_result[i][j]>present_threshold):\n",
    "            topic_set.append(idx_label[str(j)])\n",
    "            output_count[j]+=1\n",
    "    p_data[i]['topic_set']=topic_set\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# sanity check 0 this is source node\n",
    "print combine_result[790]\n",
    "# this is none source node\n",
    "print combine_result[522]\n",
    "print ' '\n",
    "\n",
    "for i in range(len(idx_label)):\n",
    "    print idx_label[str(i)]\n",
    "    print topic_count[i]\n",
    "    print output_count[i]\n",
    "    print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"data/topics_output.txt\"\n",
    "if(os.path.isfile(path)):\n",
    "    os.remove(path)\n",
    "with open(path, 'w') as f:\n",
    "    for p in p_data:\n",
    "        head=p['index']+' '+p['title']+'\\n'\n",
    "        f.write(head.encode('utf-8'))\n",
    "        ratio='prop_ratio: '+str(p['actual_ratio'])+'\\n'\n",
    "        f.write(ratio.encode('utf-8'))\n",
    "        f.write('[')\n",
    "        for i in p['topic_set']:\n",
    "            f.write(i)\n",
    "            f.write(',  ')\n",
    "        f.write(']\\n\\n')\n",
    "        f.write(p['abstract'].encode('utf-8'))\n",
    "        f.write('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preference_analysis\n",
      "[u'consumer', u'preferences', u'choice', u'sparse', u'predict', u'learning', u'car', u'purchase', u'data', u'consumer preferences', u'market', u'set', u'passenger', u'high dimensional', u'conjoint', u'network', u'accuracy', u'choice model', u'logit', u'association', u'original', u'representation', u'interaction', u'transformation', u'high', u'model', u'complement', u'negligible', u'ann', u'machine', u'survey', u'product', u'composite', u'dimensional', u'features', u'binary', u'existing', u'questions', u'behavior', u'improve', u'profile', u'restricted', u'neural', u'neural network', u'misleading', u'code', u'goal', u'community', u'actual', u'generation']\n",
      " \n",
      "algorithm_probabilistic_approach\n",
      "[u'uncertainty', u'reliability', u'probabilistic', u'reliability assessment', u'optimization', u'experimental', u'loop', u'algorithm', u'sora', u'efficient', u'robust', u'sequential', u'assessment', u'ERROR', u'epistemic', u'random', u'methodology', u'parameters', u'interval', u'model', u'computational', u'vehicle', u'cutting', u'high', u'simulation', u'method', u'cycle', u'proposed', u'problem', u'sampling', u'surrogate', u'deterministic', u'uncertain', u'probability', u'constraints', u'design', u'epistemic uncertainty', u'kriging', u'performance', u'iterative', u'points', u'robust design', u'design optimization', u'decision', u'criteria', u'aleatory', u'search', u'rbdo', u'model uncertainty', u'random field']\n",
      " \n",
      "product_system_design\n",
      "[u'mistakes', u'decentralized', u'vehicle', u'architecture', u'crash', u'convergence', u'occupant', u'surrogate', u'pulse', u'surrogate models', u'distribution', u'design process', u'problem', u'approach', u'design', u'optimization', u'time', u'subsystems', u'process', u'solutions', u'design problem', u'legacy', u'decision', u'parametric', u'response', u'market', u'impact', u'profit', u'case', u'surface', u'product', u'model', u'sora', u'method', u'reliability assessment', u'study', u'sequential', u'understanding', u'kriging model', u'sampling', u'ensemble', u'bounds', u'disturbance', u'price', u'selection', u'kriging', u'hypothetical', u'equilibrium', u'observed', u'reliability']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "top_phrases_col=[]\n",
    "for i in range(len(idx_label)):\n",
    "    hot_index=[]\n",
    "    hot_index=np.argsort(word_proba[i],0)[::-1]\n",
    "    phrases=[]\n",
    "    for j in range(50):\n",
    "        phrases.append(index_phrase[str(hot_index[j])])\n",
    "    top_phrases_col.append(phrases)\n",
    "\n",
    "    \n",
    "for i in range(len(idx_label)):    \n",
    "    print idx_label[str(i)]\n",
    "    print top_phrases_col[i]\n",
    "    print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
