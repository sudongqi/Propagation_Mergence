{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "0.936575034873\n",
      "458\n",
      "0.901034571623\n",
      "458\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#this is a variant of propagation_mergence, it only allow propagation via directed edge.\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import os\n",
    "\n",
    "\n",
    "#predefined topics size\n",
    "label_num = 180\n",
    "#orginiality determined how far belief can propagate through the network \n",
    "originality = 0.001\n",
    "# propagation and mixing\n",
    "iter_num = 300\n",
    "\n",
    "#-0.005 for style that looks like louvain method (~88 cluster)\n",
    "#-0.008 for 94 cluster\n",
    "#-0.01 for 103 cluster\n",
    "tolerance = 89\n",
    "\n",
    "\n",
    "with open(\"data/super_data_2.json\", \"r\") as f:\n",
    "    super_data = json.load(f)\n",
    "\n",
    "\n",
    "p_data=super_data['papers']\n",
    "bayes_rank=super_data['bayes_ranks']\n",
    "index_phrase=super_data['index_phrase']\n",
    "\n",
    "\n",
    "#propagation stage\n",
    "source=[]\n",
    "\n",
    "\n",
    "for i in range(label_num):\n",
    "    source.append(bayes_rank[i])\n",
    "\n",
    "\n",
    "# list for storing the index of all sources\n",
    "source_idx=[]\n",
    "# check set for preventing duplicate adding\n",
    "check=set()\n",
    "all_check=set()\n",
    "\n",
    "#build topics\n",
    "for i in range (len(source)):\n",
    "    check.add(source[i])\n",
    "    all_check.add(source[i])\n",
    "    source_idx.append(source[i])\n",
    "\n",
    "# BFS for find legit order\n",
    "ordering=[]\n",
    "\n",
    "#build depth 1 nodes, depth measure how far this nodes is from the sources\n",
    "depth_1=[]\n",
    "for i in source_idx:\n",
    "    for j in p_data[i]['cited_by']:\n",
    "        if not int(j) in check:\n",
    "            depth_1.append(int(j))\n",
    "            check.add(int(j))\n",
    "        \n",
    "ordering.append(depth_1)\n",
    "\n",
    "#recursively build all nodes\n",
    "running=True\n",
    "depth=0\n",
    "\n",
    "while running:\n",
    "    new_layer=[]\n",
    "    for i in ordering[depth]:\n",
    "        for j in p_data[i]['cited_by']:\n",
    "            if not int(j) in check:\n",
    "                check.add(int(j))\n",
    "                new_layer.append(int(j))\n",
    "    if(len(new_layer)==0):\n",
    "        running=False\n",
    "    else:\n",
    "        ordering.append(new_layer)\n",
    "        depth+=1\n",
    "\n",
    "# build edge weight by normalization\n",
    "weights=[] \n",
    "\n",
    "\n",
    "for p in p_data:\n",
    "    z = float(sum(p['citations_sim'])) + originality\n",
    "    vec=np.array(p['citations_sim'])/z\n",
    "    p['prop_ratio']=1-(originality/z)\n",
    "    weights.append(vec)\n",
    "        \n",
    "        \n",
    "# turn all str in citation into int vector\n",
    "for p in p_data:\n",
    "    int_vec=[]\n",
    "    for i in p['citations']:\n",
    "        int_vec.append(int(i))\n",
    "    p['edge_set']=int_vec\n",
    "\n",
    "    \n",
    "# build and initilize topics matrix\n",
    "topics=np.zeros((len(p_data),label_num),dtype=float)\n",
    "for i in range (len(source)):\n",
    "    topics[source[i], i] = 1\n",
    "\n",
    "    \n",
    "# propagation and mixing\n",
    "\n",
    "for i in range(iter_num):\n",
    "    for layer in ordering:\n",
    "        for node in layer:\n",
    "            n_vec = p_data[node]['edge_set']\n",
    "            sub = topics[n_vec, :]\n",
    "            update = np.dot(weights[node], sub)\n",
    "            topics[node,:]=update\n",
    "\n",
    "# analysis for the effect of propagation\n",
    "for i in range(len(p_data)):\n",
    "    p_data[i]['actual_ratio']=np.sum(topics[i])\n",
    "\n",
    "actual_ratios=[]\n",
    "prop_ratios=[]\n",
    "for p in p_data:\n",
    "    if p['prop_ratio']!=0.0:\n",
    "        prop_ratios.append(p['prop_ratio'])\n",
    "    if p['actual_ratio']!=0.0:\n",
    "        actual_ratios.append(p['actual_ratio'])\n",
    "\n",
    "        \n",
    "# prop_ratios contain node that has a least one undirected connection      \n",
    "print len(prop_ratios)\n",
    "print sum(prop_ratios)/len(prop_ratios)\n",
    "# actual_ratios contain no 0\n",
    "print len(actual_ratios)\n",
    "# actual ratios being \n",
    "print sum(actual_ratios)/len(actual_ratios)\n",
    "# number of nodes in network\n",
    "print len(check)\n",
    "# depth of the tree\n",
    "print len(ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1474\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "# merge stage\n",
    "from copy import deepcopy\n",
    "#build the initial group\n",
    "\n",
    "group=[[] for i in range(label_num)]\n",
    "\n",
    "count=0\n",
    "for i in range(len(p_data)):\n",
    "    if(np.max(topics[i])!=0):\n",
    "        p_data[i]['prop_group']=np.argmax(topics[i])\n",
    "        group[np.argmax(topics[i])].append(i)\n",
    "        count+=1\n",
    "    else:\n",
    "        p_data[i]['prop_group']=-1\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# there must be some nodes that were left over by the propagation,\n",
    "# we need to give them a unique for the mergence stage\n",
    "depth_1=[]\n",
    "for i in source_idx:\n",
    "    for j in p_data[i]['all_cite']:\n",
    "        if not int(j) in all_check:\n",
    "            depth_1.append(int(j))\n",
    "            all_check.add(int(j))\n",
    "\n",
    "ordering=[]\n",
    "ordering.append(depth_1)\n",
    "\n",
    "running=True\n",
    "depth=0\n",
    "\n",
    "while running:\n",
    "    new_layer=[]\n",
    "    for i in ordering[depth]:\n",
    "        for j in p_data[i]['all_cite']:\n",
    "            if not int(j) in all_check:\n",
    "                new_layer.append(int(j))\n",
    "                all_check.add(int(j))\n",
    "    if(len(new_layer)==0):\n",
    "        running=False\n",
    "    else:\n",
    "        ordering.append(new_layer)\n",
    "        depth+=1\n",
    "    \n",
    "extra_group_num=label_num\n",
    "for i in all_check:\n",
    "    if p_data[i]['prop_group']==-1:\n",
    "        p_data[i]['prop_group']=extra_group_num\n",
    "        group.append([i])\n",
    "        extra_group_num+=1\n",
    "\n",
    "\n",
    "print len(all_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Queue import PriorityQueue\n",
    "\n",
    "        \n",
    "# build group info\n",
    "group_info={}\n",
    "\n",
    "# keep track of the indepedent group\n",
    "curr_group=set()\n",
    "\n",
    "# structure for getting the greedy option\n",
    "q=PriorityQueue()\n",
    "\n",
    "\n",
    "# build vectors from compression\n",
    "# text vec need to fix to weight even between two abstract with different length\n",
    "def build_text_vector(vec):\n",
    "    text_vec=np.zeros(len(index_phrase))\n",
    "    for i in vec:\n",
    "        node_vec=np.zeros(len(index_phrase))\n",
    "        phrases=p_data[i]['phrases']\n",
    "        for key in phrases:\n",
    "            node_vec[int(key)]+=phrases[key]\n",
    "        node_vec/=np.sum(node_vec)\n",
    "        text_vec+=node_vec\n",
    "    text_vec/=len(vec)\n",
    "    return text_vec\n",
    "\n",
    "\n",
    "#cosine distant\n",
    "def get_similarity(a,b):\n",
    "    return np.dot(a,b)/(la.norm(a)*la.norm(b))\n",
    "\n",
    "\n",
    "node_group=[-1 for i in range(len(p_data))]\n",
    "\n",
    "# build group with info\n",
    "for i in range(len(group)):\n",
    "    group_info[str(i)]={}\n",
    "    group_info[str(i)]['nodes']=set(group[i])\n",
    "    group_info[str(i)]['text_vec']=build_text_vector(group[i])\n",
    "    group_info[str(i)]['size']=len(group[i])\n",
    "    group_info[str(i)]['meta_group']=str(i)\n",
    "    curr_group.add(str(i))\n",
    "    for j in group[i]:\n",
    "        node_group[j]=str(i)\n",
    "\n",
    "\n",
    "# build connection\n",
    "for key in group_info:\n",
    "    g=group_info[key]\n",
    "    nodes=g['nodes']\n",
    "    connected_group=set()\n",
    "    for node in nodes:\n",
    "        citation_from_node = p_data[node]['all_cite']\n",
    "        for c in citation_from_node:\n",
    "            o_group=node_group[int(c)]\n",
    "            if(o_group!=key):\n",
    "                connected_group.add(o_group)\n",
    "    g['connected_group']=connected_group\n",
    "    for cg in connected_group:\n",
    "        q.put([-get_similarity(group_info[key]['text_vec'],group_info[cg]['text_vec']),[key,cg]])\n",
    "        \n",
    "\n",
    "#get the meta group for the small group\n",
    "def get_meta(node):\n",
    "    temp=node\n",
    "    while(group_info[temp]['meta_group']!=temp):\n",
    "        # get the parent until we hit the root\n",
    "        temp=group_info[temp]['meta_group']\n",
    "    return temp\n",
    "\n",
    "\n",
    "total_degree=0\n",
    "for p in p_data:\n",
    "    total_degree+=len(p['all_cite'])\n",
    "\n",
    "#get modularity gain\n",
    "def get_modularity(a_node, b_node):    \n",
    "    delta_q=0\n",
    "    count=0\n",
    "    for n in a_node:\n",
    "        a_degree=len(p_data[n]['all_cite'])\n",
    "        for c in p_data[n]['all_cite']:\n",
    "            b_degree=len(p_data[int(c)]['all_cite'])\n",
    "            if int(c) in b_node:\n",
    "                count+=1\n",
    "                delta_q+=1-(float(a_degree*b_degree)/total_degree)\n",
    "            else:\n",
    "                delta_q+=(-(float(a_degree*b_degree)/total_degree))\n",
    "    delta_q/=total_degree\n",
    "    return delta_q\n",
    "    \n",
    "    \n",
    "    \n",
    "#merge two groups and update the data structure\n",
    "def merge_group(a, b):\n",
    "    \n",
    "    # change group_info\n",
    "    name = a+','+b\n",
    "\n",
    "    ga=group_info[a]\n",
    "    gb=group_info[b]\n",
    "    \n",
    "    #delta_q = get_modularity(ga['nodes'],gb['nodes'])\n",
    "    \n",
    "    ga['meta_group']=name\n",
    "    gb['meta_group']=name\n",
    "    group_info[name]={}\n",
    "    group_info[name]['nodes']=(ga['nodes']|gb['nodes'])\n",
    "    group_info[name]['size']=ga['size']+gb['size']\n",
    "    group_info[name]['text_vec']=(ga['text_vec']*ga['size']+gb['text_vec']*gb['size'])/group_info[name]['size']\n",
    "    group_info[name]['meta_group']=name\n",
    "    curr_group.remove(a)\n",
    "    curr_group.remove(b)\n",
    "    curr_group.add(name)\n",
    "    \n",
    "    # find new connection via meta_group, current connection could be out of dated due to new merge\n",
    "    combine_connection=(ga['connected_group']|gb['connected_group'])\n",
    "    connected_group=set()\n",
    "    \n",
    "    for i in combine_connection:\n",
    "        meta=get_meta(i)\n",
    "        if(meta!=name):\n",
    "            if(not meta in connected_group):\n",
    "                connected_group.add(meta)\n",
    "                  \n",
    "    group_info[name]['connected_group']=connected_group\n",
    "    # create new option for queue\n",
    "    for cg in connected_group:\n",
    "        penalty=1.0/(group_info[name]['size']+group_info[cg]['size'])\n",
    "        score = (get_similarity(group_info[name]['text_vec'],group_info[cg]['text_vec'])) * penalty\n",
    "        q.put([-score,[name,cg]])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# greedy merge until run out of options or reach the minimal tolerance\n",
    "while (not q.empty()):\n",
    "    option = q.get()\n",
    "    if(len(curr_group)<tolerance):\n",
    "        break\n",
    "    tup=option[1]\n",
    "    if ((not tup[0] in curr_group) or (not tup[1] in curr_group)):\n",
    "        continue \n",
    "    merge_group(tup[0],tup[1])\n",
    "\n",
    "    \n",
    "# organize group info \n",
    "group_index={}\n",
    "final_group_info=[]\n",
    "\n",
    "# produce a new index for the merged groups\n",
    "count=0\n",
    "for group in curr_group:\n",
    "    group_index[group]=count\n",
    "    group_info[group]['index']=count\n",
    "    final_group_info.append(group_info[group])\n",
    "    count+=1\n",
    "    \n",
    "# update the connection in meta group with integer index\n",
    "for group in curr_group:\n",
    "    new_connection=set()\n",
    "    for cg in group_info[group]['connected_group']:\n",
    "        new_connection.add(group_info[get_meta(cg)]['index'])\n",
    "    group_info[group]['connected_group']=new_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# update all node ppm index\n",
    "for p in p_data:\n",
    "    p['ppm_index']=-1\n",
    "\n",
    "        \n",
    "for group in final_group_info:\n",
    "    nodes = group['nodes']\n",
    "    for n in nodes:\n",
    "        p_data[n]['ppm_index']=group_index[group['meta_group']]\n",
    "\n",
    "        \n",
    "\n",
    "#Top phrase for every group\n",
    "group_phrases=[]\n",
    "for i in range(len(group_index)):\n",
    "    top_phrase=[]\n",
    "    b=np.argsort(final_group_info[i]['text_vec'])[::-1]\n",
    "    for k in range(30):\n",
    "        top_phrase.append(index_phrase[str(b[k])])\n",
    "    final_group_info[i]['top_phrase']=top_phrase\n",
    "    name_str = (top_phrase[0]+', '+top_phrase[1]+' and '+top_phrase[2])\n",
    "    final_group_info[i]['name']=name_str\n",
    "\n",
    "# get importer, exporter and contribution score\n",
    "for group in final_group_info:\n",
    "    \n",
    "    index=group['index']\n",
    "    #map group to node\n",
    "    importer={}\n",
    "    exporter={}\n",
    "    #map group to number\n",
    "    import_score={}\n",
    "    export_score={}\n",
    "    exchange_score={}\n",
    "\n",
    "    \n",
    "    for cg in group['connected_group']:\n",
    "        importer[cg]=set()\n",
    "        exporter[cg]=set()\n",
    "        import_score[cg]=0\n",
    "        export_score[cg]=0\n",
    "        exchange_score[cg]=0\n",
    "    \n",
    "    for node in group['nodes']:\n",
    "        \n",
    "        for c in p_data[node]['citations']:\n",
    "            out_index=p_data[int(c)]['ppm_index']\n",
    "            if(out_index!=index):\n",
    "                importer[out_index].add(node)\n",
    "                import_score[out_index]+=1\n",
    "                exchange_score[out_index]+=1\n",
    "                \n",
    "        for c in p_data[node]['cited_by']:\n",
    "            out_index=p_data[int(c)]['ppm_index']\n",
    "            if(out_index!=index):\n",
    "                exporter[out_index].add(node)\n",
    "                export_score[out_index]+=1\n",
    "                exchange_score[out_index]+=1\n",
    "                \n",
    "    for cg in group['connected_group']:\n",
    "        importer[cg]=list(importer[cg])\n",
    "        exporter[cg]=list(exporter[cg])\n",
    "    \n",
    "    import_list=[]\n",
    "    for key in import_score:\n",
    "        import_list.append([key, import_score[key]])\n",
    "    export_list=[]\n",
    "    for key in export_score:\n",
    "        export_list.append([key, export_score[key]])\n",
    "    exchange_list=[]\n",
    "    for key in exchange_score:\n",
    "        exchange_list.append([key, exchange_score[key]])\n",
    "    \n",
    "    group['import_list']=sorted(import_list,key=itemgetter(1),reverse=True)\n",
    "    group['export_list']=sorted(export_list,key=itemgetter(1),reverse=True)\n",
    "    group['exchange_list']=sorted(exchange_list,key=itemgetter(1),reverse=True)\n",
    "    \n",
    "    group['importer']=importer\n",
    "    group['exporter']=exporter\n",
    "\n",
    "\n",
    "# turn set into list for storage\n",
    "for group in final_group_info:\n",
    "    group['nodes']=list(group['nodes'])\n",
    "    group['connected_group']=list(group['connected_group'])\n",
    "    del group['text_vec']\n",
    "    del group['meta_group']\n",
    "\n",
    "super_data['ppm_group']=final_group_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"data/super_data_3.json\" \n",
    "if(os.path.isfile(path)): \n",
    "    os.remove(path) \n",
    "with open(path, \"w\") as f: \n",
    "    json.dump(super_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "69\n",
      " \n",
      "135\n",
      "166\n",
      "178,24,156,123,21\n",
      "22\n",
      "23\n",
      "115,77\n",
      "18,34,33,40,1,167,176,150,28,11,29,141,65\n",
      "121\n",
      "125\n",
      "126\n",
      "118\n",
      "120,158\n",
      "151,56\n",
      "201\n",
      "198,2,113,19,46,170,52,112,49,62,37,122,189,47,196,182,142\n",
      "194\n",
      "197\n",
      "137,177\n",
      "190\n",
      "193\n",
      "88\n",
      "89\n",
      "82\n",
      "83\n",
      "86\n",
      "87\n",
      "84\n",
      "85\n",
      "130,157\n",
      "108\n",
      "109\n",
      "200,51\n",
      "102\n",
      "103\n",
      "100\n",
      "101\n",
      "106\n",
      "107\n",
      "104\n",
      "105\n",
      "127,48\n",
      "17,80,68,20,43,70,78,128,53,139,4,61,124,74,8,192,36,6,81,114,12,138,0,72\n",
      "155,30\n",
      "31,7,79,154,148,147,160,44,134,163,149,165,181,15,3,60,25,38,13,14,50,27,9,185,171,58,179,26,55,57\n",
      "66\n",
      "173\n",
      "172\n",
      "203\n",
      "39,64,195,161\n",
      "180\n",
      "187\n",
      "111,75,16,32,191,184,133,199\n",
      "186\n",
      "188\n",
      "202\n",
      "117,183\n",
      "153,67,144,146,152\n",
      "99\n",
      "98\n",
      "168\n",
      "169\n",
      "129,204\n",
      "91\n",
      "90\n",
      "93\n",
      "92\n",
      "95\n",
      "94\n",
      "97\n",
      "96\n",
      "110,162\n",
      "10\n",
      "116\n",
      "159\n",
      "41,59,136,54\n",
      "119,174\n",
      "45\n",
      "132,175\n",
      "5\n",
      "131,69,164\n",
      "145\n",
      "143\n",
      "140\n",
      "76\n",
      "73\n",
      "71\n",
      "35,42,63\n"
     ]
    }
   ],
   "source": [
    "#check if the the clusters are balance (the number below are the basic clusters)\n",
    "\n",
    "print (len(super_data['ppm_group']))\n",
    "print (len(super_data['louvain_group']))\n",
    "\n",
    "\n",
    "print ' '\n",
    "for c in curr_group:\n",
    "    print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
